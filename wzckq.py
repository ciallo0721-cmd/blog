import tkinter as tk
from tkinter import ttk, scrolledtext, filedialog, messagebox
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, parse_qs
import threading
import queue
import time
from datetime import datetime
import json
import os
import re
import mimetypes
from concurrent.futures import ThreadPoolExecutor, as_completed

class WebsiteScannerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("æ™ºèƒ½ç½‘ç«™ç›®å½•æ‰«æå™¨ v2.1")
        self.root.geometry("1000x800")
        
        # æ‰«ææ§åˆ¶
        self.scanning = False
        self.stop_flag = False
        self.scanned_urls = set()
        self.result_queue = queue.Queue()
        self.target_domain = ""
        
        # åˆ›å»ºGUI
        self.create_widgets()
        
        # å¯åŠ¨ç»“æœæ›´æ–°çº¿ç¨‹
        self.update_thread = threading.Thread(target=self.update_results, daemon=True)
        self.update_thread.start()
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.load_config()

    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        self.config = {
            'common_extensions': [
                '.html', '.htm', '.php', '.asp', '.aspx', '.jsp', '.cgi',
                '.js', '.css', '.json', '.xml', '.txt', '.md', '.pdf',
                '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
                '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.ico', '.webp',
                '.mp3', '.mp4', '.avi', '.mov', '.wav', '.flv', '.m3u8',
                '.zip', '.rar', '.tar', '.gz', '.7z', '.bz2',
                '.eot', '.ttf', '.woff', '.woff2', '.otf'
            ],
            'ignore_patterns': [
                r'^#.*$',  # é”šç‚¹
                r'^javascript:',  # JavaScriptä»£ç 
                r'^mailto:',  # é‚®ä»¶é“¾æ¥
                r'^tel:',  # ç”µè¯é“¾æ¥
                r'^data:',  # Data URL
                r'^blob:',  # Blob URL
                r'^ws[s]?://',  # WebSocketé“¾æ¥
            ],
            'ignore_extensions': [
                '.exe', '.dll', '.bin', '.so', '.dylib',  # å¯æ‰§è¡Œæ–‡ä»¶
                '.db', '.sqlite', '.mdb',  # æ•°æ®åº“
                '.log', '.tmp', '.temp', '.cache',  # ä¸´æ—¶æ–‡ä»¶
            ],
            'ignore_params': [
                'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
                'fbclid', 'gclid', 'msclkid', 'dclid',  # å¹¿å‘Šè¿½è¸ªå‚æ•°
                'ref', 'source', 'campaign', 'medium', 'term', 'content'
            ],
            'user_agents': [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
            ],
            'timeout': 15,
            'max_redirects': 5,
            'max_workers': 5
        }
        
        # åŠ è½½è‡ªå®šä¹‰æ‰©å±•ååˆ—è¡¨
        self.custom_extensions = []
        try:
            if os.path.exists('extensions.txt'):
                with open('extensions.txt', 'r', encoding='utf-8') as f:
                    self.custom_extensions = [ext.strip() for ext in f.readlines() if ext.strip()]
        except:
            pass

    def create_widgets(self):
        """åˆ›å»ºGUIç»„ä»¶"""
        # åˆ›å»ºä¸»æ¡†æ¶
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # é…ç½®ç½‘æ ¼æƒé‡
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        
        # æ§åˆ¶åŒºåŸŸ
        control_frame = ttk.LabelFrame(main_frame, text="æ‰«æè®¾ç½®", padding="10")
        control_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        control_frame.columnconfigure(1, weight=1)
        
        # URLè¾“å…¥
        ttk.Label(control_frame, text="ç›®æ ‡URL:").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        self.url_var = tk.StringVar(value="https://ciallo0721-cmd.github.io/blog/")
        self.url_entry = ttk.Entry(control_frame, textvariable=self.url_var, width=70)
        self.url_entry.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(0, 10), columnspan=3)
        
        # æ·±åº¦è®¾ç½®
        ttk.Label(control_frame, text="æ‰«ææ·±åº¦:").grid(row=1, column=0, sticky=tk.W, padx=(0, 5))
        self.depth_var = tk.IntVar(value=2)
        self.depth_spinbox = ttk.Spinbox(control_frame, from_=1, to=10, textvariable=self.depth_var, width=10)
        self.depth_spinbox.grid(row=1, column=1, sticky=tk.W, padx=(0, 10))
        
        # çº¿ç¨‹æ•°è®¾ç½®
        ttk.Label(control_frame, text="å¹¶å‘çº¿ç¨‹:").grid(row=2, column=0, sticky=tk.W, padx=(0, 5))
        self.threads_var = tk.IntVar(value=5)
        self.threads_spinbox = ttk.Spinbox(control_frame, from_=1, to=20, textvariable=self.threads_var, width=10)
        self.threads_spinbox.grid(row=2, column=1, sticky=tk.W, padx=(0, 10))
        
        # æ‰«æé€‰é¡¹
        options_frame = ttk.Frame(control_frame)
        options_frame.grid(row=1, column=2, rowspan=2, padx=20, sticky=tk.W)
        
        self.scan_same_domain = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="ä»…æ‰«æåŒåŸŸå", variable=self.scan_same_domain).grid(row=0, column=0, sticky=tk.W)
        
        self.remove_query = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="å»é™¤URLå‚æ•°", variable=self.remove_query).grid(row=0, column=1, sticky=tk.W, padx=(10, 0))
        
        self.detect_hidden = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="æ¢æµ‹éšè—æ–‡ä»¶", variable=self.detect_hidden).grid(row=1, column=0, sticky=tk.W)
        
        self.verbose_log = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="è¯¦ç»†æ—¥å¿—", variable=self.verbose_log).grid(row=1, column=1, sticky=tk.W, padx=(10, 0))
        
        # æ§åˆ¶æŒ‰é’®æ¡†æ¶
        button_frame = ttk.Frame(control_frame)
        button_frame.grid(row=3, column=0, columnspan=4, pady=(10, 0), sticky=(tk.W, tk.E))
        
        self.start_btn = ttk.Button(button_frame, text="â–¶ å¼€å§‹æ‰«æ", command=self.start_scan, width=12)
        self.start_btn.grid(row=0, column=0, padx=(0, 5))
        
        self.stop_btn = ttk.Button(button_frame, text="â¹ åœæ­¢æ‰«æ", command=self.stop_scan, state=tk.DISABLED, width=12)
        self.stop_btn.grid(row=0, column=1, padx=5)
        
        self.save_btn = ttk.Button(button_frame, text="ğŸ’¾ ä¿å­˜ç»“æœ", command=self.save_results, state=tk.DISABLED, width=12)
        self.save_btn.grid(row=0, column=2, padx=5)
        
        self.clear_btn = ttk.Button(button_frame, text="ğŸ—‘ï¸ æ¸…ç©ºç»“æœ", command=self.clear_results, width=12)
        self.clear_btn.grid(row=0, column=3, padx=5)
        
        self.export_btn = ttk.Button(button_frame, text="ğŸ“‹ å¯¼å‡ºåˆ—è¡¨", command=self.export_list, state=tk.DISABLED, width=12)
        self.export_btn.grid(row=0, column=4, padx=5)
        
        # ç»“æœåŒºåŸŸ
        result_frame = ttk.LabelFrame(main_frame, text="æ‰«æç»“æœ", padding="10")
        result_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        result_frame.columnconfigure(0, weight=1)
        result_frame.rowconfigure(0, weight=1)
        
        # åˆ›å»ºæ ‡ç­¾é¡µ
        self.notebook = ttk.Notebook(result_frame)
        self.notebook.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # æ ‘çŠ¶ç»“æ„æ ‡ç­¾é¡µ
        tree_frame = ttk.Frame(self.notebook)
        self.tree_text = scrolledtext.ScrolledText(tree_frame, wrap=tk.WORD, width=120, height=25)
        self.tree_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        tree_frame.columnconfigure(0, weight=1)
        tree_frame.rowconfigure(0, weight=1)
        self.notebook.add(tree_frame, text="ç›®å½•ç»“æ„")
        
        # æ–‡ä»¶åˆ—è¡¨æ ‡ç­¾é¡µ
        list_frame = ttk.Frame(self.notebook)
        self.list_text = scrolledtext.ScrolledText(list_frame, wrap=tk.WORD, width=120, height=25)
        self.list_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        list_frame.columnconfigure(0, weight=1)
        list_frame.rowconfigure(0, weight=1)
        self.notebook.add(list_frame, text="æ–‡ä»¶åˆ—è¡¨")
        
        # æ—¥å¿—æ ‡ç­¾é¡µ
        log_frame = ttk.Frame(self.notebook)
        self.log_text = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, width=120, height=25)
        self.log_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        log_frame.columnconfigure(0, weight=1)
        log_frame.rowconfigure(0, weight=1)
        self.notebook.add(log_frame, text="æ‰«ææ—¥å¿—")
        
        # çŠ¶æ€æ 
        self.status_var = tk.StringVar(value="å°±ç»ª")
        self.status_bar = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN)
        self.status_bar.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E))
        
        # è¿›åº¦æ¡å’Œç»Ÿè®¡
        info_frame = ttk.Frame(main_frame)
        info_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 5))
        
        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(info_frame, variable=self.progress_var, maximum=100, length=300)
        self.progress_bar.grid(row=0, column=0, sticky=tk.W)
        
        self.stats_var = tk.StringVar(value="å·²æ‰«æ: 0 | ç›®å½•: 0 | æ–‡ä»¶: 0 | è€—æ—¶: 0s")
        self.stats_label = ttk.Label(info_frame, textvariable=self.stats_var)
        self.stats_label.grid(row=0, column=1, sticky=tk.E, padx=(10, 0))

    def start_scan(self):
        """å¼€å§‹æ‰«æ"""
        url = self.url_var.get().strip()
        if not url:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥è¦æ‰«æçš„URL")
            return
        
        # æ·»åŠ åè®®å‰ç¼€
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        self.url_var.set(url)
        
        # è§£æç›®æ ‡åŸŸå
        try:
            parsed = urlparse(url)
            self.target_domain = parsed.netloc
            if not self.target_domain:
                messagebox.showerror("é”™è¯¯", "æ— æ•ˆçš„URL")
                return
        except:
            messagebox.showerror("é”™è¯¯", "æ— æ•ˆçš„URL")
            return
        
        # é‡ç½®çŠ¶æ€
        self.scanning = True
        self.stop_flag = False
        self.scanned_urls.clear()
        
        # æ¸…ç©ºæ˜¾ç¤ºåŒºåŸŸ
        self.tree_text.delete(1.0, tk.END)
        self.list_text.delete(1.0, tk.END)
        self.log_text.delete(1.0, tk.END)
        
        # æ›´æ–°UIçŠ¶æ€
        self.start_btn.config(state=tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL)
        self.save_btn.config(state=tk.DISABLED)
        self.export_btn.config(state=tk.DISABLED)
        
        # å¼€å§‹æ‰«æçº¿ç¨‹
        scan_thread = threading.Thread(target=self.scan_website, args=(url,))
        scan_thread.daemon = True
        scan_thread.start()

    def stop_scan(self):
        """åœæ­¢æ‰«æ"""
        self.stop_flag = True
        self.scanning = False
        self.status_var.set("æ­£åœ¨åœæ­¢...")
        self.log("ç”¨æˆ·åœæ­¢äº†æ‰«æ")

    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        filename = filedialog.asksaveasfilename(
            defaultextension=".txt",
            filetypes=[
                ("Text files", "*.txt"), 
                ("JSON files", "*.json"), 
                ("HTML files", "*.html"),
                ("All files", "*.*")
            ]
        )
        
        if filename:
            try:
                if filename.endswith('.json'):
                    self.save_as_json(filename)
                elif filename.endswith('.html'):
                    self.save_as_html(filename)
                else:
                    self.save_as_text(filename)
                messagebox.showinfo("æˆåŠŸ", f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            except Exception as e:
                messagebox.showerror("é”™è¯¯", f"ä¿å­˜å¤±è´¥: {str(e)}")

    def save_as_text(self, filename):
        """ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶"""
        content = self.tree_text.get(1.0, tk.END)
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)

    def save_as_json(self, filename):
        """ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
        results = {
            'target_url': self.url_var.get(),
            'scan_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'directories': self.directories if hasattr(self, 'directories') else [],
            'files': self.files if hasattr(self, 'files') else []
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    def save_as_html(self, filename):
        """ä¿å­˜ä¸ºHTMLæ–‡ä»¶"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>ç½‘ç«™æ‰«æç»“æœ - {self.url_var.get()}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1 {{ color: #333; }}
                .tree {{ font-family: monospace; }}
                .dir {{ color: #007bff; }}
                .file {{ color: #28a745; }}
                .stats {{ background: #f8f9fa; padding: 10px; border-radius: 5px; }}
            </style>
        </head>
        <body>
            <h1>ç½‘ç«™æ‰«æç»“æœ</h1>
            <div class="stats">
                <p><strong>ç›®æ ‡URL:</strong> {self.url_var.get()}</p>
                <p><strong>æ‰«ææ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            <pre class="tree">{self.tree_text.get(1.0, tk.END)}</pre>
        </body>
        </html>
        """
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html_content)

    def export_list(self):
        """å¯¼å‡ºæ–‡ä»¶åˆ—è¡¨"""
        filename = filedialog.asksaveasfilename(
            defaultextension=".txt",
            filetypes=[("Text files", "*.txt"), ("CSV files", "*.csv"), ("All files", "*.*")]
        )
        
        if filename:
            content = self.list_text.get(1.0, tk.END)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            messagebox.showinfo("æˆåŠŸ", f"æ–‡ä»¶åˆ—è¡¨å·²å¯¼å‡ºåˆ°: {filename}")

    def clear_results(self):
        """æ¸…ç©ºç»“æœ"""
        self.tree_text.delete(1.0, tk.END)
        self.list_text.delete(1.0, tk.END)
        self.log_text.delete(1.0, tk.END)
        self.stats_var.set("å·²æ‰«æ: 0 | ç›®å½•: 0 | æ–‡ä»¶: 0 | è€—æ—¶: 0s")
        self.progress_var.set(0)

    def log(self, message):
        """æ·»åŠ æ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.result_queue.put(f"LOG:{timestamp} {message}\n")

    def scan_website(self, url):
        """æ‰«æç½‘ç«™çš„ä¸»å‡½æ•°"""
        try:
            self.start_time = time.time()
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            # åˆå§‹åŒ–ç»“æœ
            self.directories = []
            self.files = []
            self.external_links = []
            
            self.log(f"å¼€å§‹æ‰«æ: {url}")
            self.log(f"ç›®æ ‡åŸŸå: {self.target_domain}")
            
            # æ‰«æåˆå§‹URL
            self.scan_page(url, 0, base_url)
            
            # å¤šçº¿ç¨‹æ‰«æå…¶ä»–é¡µé¢
            for depth in range(1, self.depth_var.get() + 1):
                if self.stop_flag:
                    break
                    
                # å‡†å¤‡è¦æ‰«æçš„ç›®å½•
                dirs_to_scan = []
                for dir_url in self.directories:
                    if dir_url not in self.scanned_urls and self.is_same_domain(dir_url):
                        dirs_to_scan.append(dir_url)
                
                if not dirs_to_scan:
                    break
                
                self.log(f"æ·±åº¦ {depth}: å‡†å¤‡æ‰«æ {len(dirs_to_scan)} ä¸ªç›®å½•")
                
                # å¤šçº¿ç¨‹æ‰«æ
                with ThreadPoolExecutor(max_workers=self.threads_var.get()) as executor:
                    futures = []
                    for dir_url in dirs_to_scan:
                        if self.stop_flag:
                            break
                        future = executor.submit(self.scan_page, dir_url, depth, base_url)
                        futures.append(future)
                    
                    # æ”¶é›†ç»“æœ
                    for i, future in enumerate(as_completed(futures), 1):
                        if self.stop_flag:
                            break
                        try:
                            future.result(timeout=30)
                        except Exception as e:
                            self.log(f"æ‰«æå‡ºé”™: {str(e)[:50]}")
                
                # æ›´æ–°è¿›åº¦
                progress = min(100, (depth / self.depth_var.get()) * 100)
                self.progress_var.set(progress)
                
                elapsed = time.time() - self.start_time
                self.stats_var.set(f"å·²æ‰«æ: {len(self.scanned_urls)} | ç›®å½•: {len(self.directories)} | æ–‡ä»¶: {len(self.files)} | è€—æ—¶: {elapsed:.1f}s")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            if not self.stop_flag:
                self.display_results(base_url)
            
        except Exception as e:
            self.log(f"æ‰«æå‡ºé”™: {str(e)}")
        finally:
            self.scanning = False
            self.root.after(0, self.on_scan_complete)

    def scan_page(self, url, depth, base_url):
        """æ‰«æå•ä¸ªé¡µé¢"""
        if url in self.scanned_urls or self.stop_flag:
            return
        
        try:
            self.scanned_urls.add(url)
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': self.config['user_agents'][depth % len(self.config['user_agents'])],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Referer': base_url,
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(
                url, 
                headers=headers, 
                timeout=self.config['timeout'],
                allow_redirects=True,
                verify=False
            )
            response.raise_for_status()
            
            # è®°å½•æˆåŠŸ
            if self.verbose_log.get():
                self.log(f"æˆåŠŸ: {url} ({response.status_code})")
            
            # è§£æå†…å®¹
            content_type = response.headers.get('content-type', '').lower()
            
            if 'text/html' in content_type:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                links = []
                
                # <a> æ ‡ç­¾
                for tag in soup.find_all('a', href=True):
                    links.append(('href', tag['href']))
                
                # <link> æ ‡ç­¾
                for tag in soup.find_all('link', href=True):
                    links.append(('href', tag['href']))
                
                # <script> æ ‡ç­¾
                for tag in soup.find_all('script', src=True):
                    links.append(('src', tag['src']))
                
                # <img> æ ‡ç­¾
                for tag in soup.find_all('img', src=True):
                    links.append(('src', tag['src']))
                
                # <source> æ ‡ç­¾
                for tag in soup.find_all('source', src=True):
                    links.append(('src', tag['src']))
                
                # å¤„ç†é“¾æ¥
                for attr_type, href in links:
                    self.process_link(url, href, depth, base_url)
                
                # æŸ¥æ‰¾CSSä¸­çš„URL
                for style in soup.find_all('style'):
                    if style.string:
                        urls = re.findall(r'url\(["\']?([^"\')]+)["\']?\)', style.string)
                        for url_found in urls:
                            self.process_link(url, url_found, depth, base_url)
                
                # æŸ¥æ‰¾JavaScriptä¸­çš„URL
                for script in soup.find_all('script'):
                    if script.string:
                        urls = re.findall(r'["\'](https?://[^"\']+)["\']', script.string)
                        for url_found in urls:
                            self.process_link(url, url_found, depth, base_url)
            
            # å¦‚æœæ˜¯CSSæˆ–JSæ–‡ä»¶ï¼Œä¹ŸæŸ¥æ‰¾å…¶ä¸­çš„URL
            elif 'text/css' in content_type:
                urls = re.findall(r'url\(["\']?([^"\')]+)["\']?\)', response.text)
                for url_found in urls:
                    self.process_link(url, url_found, depth, base_url)
            
        except requests.exceptions.RequestException as e:
            if self.verbose_log.get():
                self.log(f"è¯·æ±‚å¤±è´¥: {url} - {str(e)[:50]}")
        except Exception as e:
            if self.verbose_log.get():
                self.log(f"è§£æå¤±è´¥: {url} - {str(e)[:50]}")

    def is_same_domain(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å±äºåŒä¸€åŸŸå"""
        if not self.scan_same_domain.get():
            return True
        
        try:
            parsed = urlparse(url)
            return parsed.netloc == self.target_domain or parsed.netloc == ''
        except:
            return False

    def process_link(self, base_url, href, depth, root_url):
        """å¤„ç†å•ä¸ªé“¾æ¥"""
        # è·³è¿‡ç©ºé“¾æ¥
        if not href or href.strip() == '':
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¿½ç•¥
        for pattern in self.config['ignore_patterns']:
            if re.match(pattern, href, re.IGNORECASE):
                return
        
        # æ„å»ºå®Œæ•´URL
        full_url = urljoin(base_url, href)
        
        # å»é™¤ç‰‡æ®µæ ‡è¯†ç¬¦
        full_url = full_url.split('#')[0]
        
        # å»é™¤æŸ¥è¯¢å‚æ•°
        if self.remove_query.get():
            parsed = urlparse(full_url)
            if parsed.query:
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¿ç•™æŸäº›å‚æ•°
                query_params = parse_qs(parsed.query)
                filtered_params = {}
                for key, values in query_params.items():
                    if key not in self.config['ignore_params']:
                        filtered_params[key] = values
                
                if filtered_params:
                    new_query = '&'.join([f"{key}={value[0]}" for key, values in filtered_params.items() for value in values])
                    full_url = parsed._replace(query=new_query).geturl()
                else:
                    full_url = parsed._replace(query='').geturl()
        
        # æ£€æŸ¥æ˜¯å¦åŒä¸€åŸŸå
        if not self.is_same_domain(full_url):
            if full_url not in self.external_links:
                self.external_links.append(full_url)
            return
        
        # æ£€æŸ¥æ˜¯å¦ä»¥/ç»“å°¾ï¼ˆå¯èƒ½æ˜¯ç›®å½•ï¼‰
        parsed = urlparse(full_url)
        path = parsed.path
        
        # æ£€æŸ¥æ‰©å±•å
        ext = os.path.splitext(path)[1].lower()
        
        # å¦‚æœæ˜¯å¸¸è§æ‰©å±•åæˆ–è€…æ˜¯æ–‡ä»¶
        if ext in self.config['common_extensions'] or ext in self.custom_extensions:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥çš„æ‰©å±•å
            if ext in self.config['ignore_extensions']:
                return
            
            # æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
            if full_url not in self.files:
                self.files.append(full_url)
                if self.verbose_log.get():
                    self.log(f"å‘ç°æ–‡ä»¶: {full_url}")
        else:
            # æ²¡æœ‰æ‰©å±•åæˆ–ä»¥/ç»“å°¾ï¼Œå¯èƒ½æ˜¯ç›®å½•
            if path.endswith('/') or not '.' in path.split('/')[-1]:
                if full_url not in self.directories:
                    self.directories.append(full_url)
                    if self.verbose_log.get():
                        self.log(f"å‘ç°ç›®å½•: {full_url}")
            else:
                # æ²¡æœ‰æ‰©å±•åä½†ä¹Ÿä¸æ˜¯ç›®å½•ï¼Œå¯èƒ½æ˜¯ä¸å¸¸è§çš„æ–‡ä»¶
                if full_url not in self.files:
                    self.files.append(full_url)
                    if self.verbose_log.get():
                        self.log(f"å‘ç°æ–‡ä»¶(æ— æ‰©å±•å): {full_url}")

    def display_results(self, base_url):
        """æ˜¾ç¤ºæ‰«æç»“æœ"""
        elapsed_time = time.time() - self.start_time
        
        # è¿‡æ»¤å¹¶æ’åºç»“æœ
        same_domain_dirs = [d for d in self.directories if self.is_same_domain(d)]
        same_domain_files = [f for f in self.files if self.is_same_domain(f)]
        
        same_domain_dirs.sort()
        same_domain_files.sort()
        
        # æ„å»ºæ ‘çŠ¶ç»“æ„
        tree = {'_type': 'root'}
        
        for url in same_domain_dirs + same_domain_files:
            parsed = urlparse(url)
            path = parsed.path.strip('/')
            
            if not path:
                continue
                
            parts = path.split('/')
            current = tree
            
            for i, part in enumerate(parts):
                if part not in current:
                    if i == len(parts) - 1:
                        # æœ€åä¸€éƒ¨åˆ†
                        current[part] = {'_type': 'dir' if url in same_domain_dirs else 'file', '_url': url}
                    else:
                        current[part] = {}
                elif i == len(parts) - 1:
                    # å·²å­˜åœ¨ï¼Œæ›´æ–°ç±»å‹
                    current[part]['_type'] = 'dir' if url in same_domain_dirs else 'file'
                    current[part]['_url'] = url
                
                current = current[part]
        
        # è¾“å‡ºæ ‘çŠ¶ç»“æ„
        self.tree_text.delete(1.0, tk.END)
        self.print_tree(base_url, tree, 0)
        
        # è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
        self.list_text.delete(1.0, tk.END)
        for i, file in enumerate(same_domain_files, 1):
            self.list_text.insert(tk.END, f"{i:3d}. {file}\n")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.stats_var.set(f"æ‰«æå®Œæˆ! å·²æ‰«æ: {len(self.scanned_urls)} | ç›®å½•: {len(same_domain_dirs)} | æ–‡ä»¶: {len(same_domain_files)} | è€—æ—¶: {elapsed_time:.1f}s")
        
        # è®°å½•åˆ°æ—¥å¿—
        self.log(f"æ‰«æå®Œæˆ! å‘ç° {len(same_domain_dirs)} ä¸ªç›®å½•, {len(same_domain_files)} ä¸ªæ–‡ä»¶")
        self.log(f"å¤–éƒ¨é“¾æ¥: {len(self.external_links)} ä¸ª")
        
        if self.external_links and self.verbose_log.get():
            self.log("å¤–éƒ¨é“¾æ¥:")
            for link in self.external_links[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                self.log(f"  - {link}")
            if len(self.external_links) > 10:
                self.log(f"  ... è¿˜æœ‰ {len(self.external_links) - 10} ä¸ªå¤–éƒ¨é“¾æ¥")

    def print_tree(self, base_url, node, depth, is_last=False, prefix=""):
        """æ‰“å°ç›®å½•æ ‘"""
        if depth == 0:
            # æ ¹èŠ‚ç‚¹
            self.tree_text.insert(tk.END, f"ğŸ“ {base_url}/\n")
            items = [(k, v) for k, v in node.items() if k != '_type']
        else:
            items = [(k, v) for k, v in node.items() if k not in ['_type', '_url']]
        
        items.sort(key=lambda x: (0 if isinstance(x[1], dict) and '_type' in x[1] and x[1]['_type'] == 'dir' else 1, x[0]))
        
        for i, (name, value) in enumerate(items):
            is_last_item = (i == len(items) - 1)
            
            if depth == 0:
                current_prefix = ""
            else:
                current_prefix = prefix + ("â””â”€â”€ " if is_last else "â”œâ”€â”€ ")
            
            if isinstance(value, dict) and '_type' in value:
                if value['_type'] == 'dir':
                    self.tree_text.insert(tk.END, f"{current_prefix}ğŸ“ {name}/\n")
                else:
                    self.tree_text.insert(tk.END, f"{current_prefix}ğŸ“„ {name}\n")
            else:
                self.tree_text.insert(tk.END, f"{current_prefix}ğŸ“ {name}/\n")
                
                # é€’å½’æ‰“å°å­èŠ‚ç‚¹
                new_prefix = prefix + ("    " if is_last else "â”‚   ")
                self.print_tree(base_url, value, depth + 1, is_last_item, new_prefix)

    def update_results(self):
        """æ›´æ–°ç»“æœæ–‡æœ¬æ¡†"""
        while True:
            try:
                result = self.result_queue.get_nowait()
                if result.startswith("LOG:"):
                    self.log_text.insert(tk.END, result[4:])
                    self.log_text.see(tk.END)
                else:
                    # å…¶ä»–ç±»å‹çš„ç»“æœ
                    pass
                self.root.update_idletasks()
            except queue.Empty:
                time.sleep(0.1)
            except:
                break

    def on_scan_complete(self):
        """æ‰«æå®Œæˆåçš„å¤„ç†"""
        self.scanning = False
        self.start_btn.config(state=tk.NORMAL)
        self.stop_btn.config(state=tk.DISABLED)
        self.save_btn.config(state=tk.NORMAL)
        self.export_btn.config(state=tk.NORMAL)
        
        if self.stop_flag:
            self.status_var.set("å·²åœæ­¢")
            self.log("æ‰«æè¢«ç”¨æˆ·åœæ­¢")
        else:
            self.status_var.set("æ‰«æå®Œæˆ")
            self.progress_var.set(100)
            self.log("æ‰«æå®Œæˆï¼Œå¯ä»¥ä¿å­˜æˆ–å¯¼å‡ºç»“æœ")

def main():
    root = tk.Tk()
    app = WebsiteScannerGUI(root)
    root.mainloop()

if __name__ == "__main__":
    main()
